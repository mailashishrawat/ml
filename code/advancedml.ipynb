{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning algorithms are a subset of artificial intelligence (AI) that imitates the human learning process. Humans learn through multiple experiences how to perform a task. Similarly, machine learning algorithms develop multiple models (usually using multiple datasets) and each model is analogous to an experience. For example, consider someone trying to learn tennis. Getting the service right requires a lot of practice, and more so to learn to serve an ace (serve such that the opponent player is unable to reach the ball). To master the service in tennis (especially ace), a player must probably practice several hundred times; each practice session is a learning. In machine learning algorithms, we develop several models which can run into several hundred and each data and model is treated as learning opportunity. Mitchell (2006) defined machine learning as follows:\n",
    "\n",
    "# Machine learns with respect to a particular task T, performance metric P following experience E, if the system reliably improves its performance P at task T following experience E.\n",
    "\n",
    "# Let the task T be a classification problem. To be more specific, consider a customer‚Äôs propensity to buy a product. The performance P can be measured through several metrics such as overall accuracy, sensitivity, specificity, and area under the receive operating characteristic curve (AUC). The experience E is analogous to different classifiers generated in machine learning algorithms such as random forest (in random forest several trees are generated, and each tree is used for classification of a new case).\n",
    "\n",
    "# The major difference between statistical learning and machine learning is that statistical learning depends heavily on validation of model assumptions and hypothesis testing, whereas the objective of machine learning is to improve prediction accuracy. For example, while developing a regression model, we check for assumptions such as normality of residuals, significance of regression parameters and so on. However, in the case of the random forest using classification trees, the most important objective is the accuracy/performance of the model.\n",
    "\n",
    "# In this module, we will discuss the following two ML algorithms:\n",
    "\n",
    "# 1. Supervised Learning: In supervised learning, the datasets have the values of input variables (feature values) and the corresponding outcome variable. The algorithms learn from the training dataset and predict the outcome variable for a new record with values of input variables. Linear regression and logistic regression are examples of supervised learning algorithms.\n",
    "\n",
    "# 2. Unsupervised Learning: In this case, the datasets will have only input variable values, but not the output. The algorithm learns the structure in the input\n",
    "\n",
    "# All ML tasks are classified into two categories: \n",
    "\n",
    "# Supervised Learning:\n",
    "\n",
    "# In supervised learning, the datasets have the values of input variables (feature values) and the corresponding outcome variable. The algorithms learn from the training dataset and predict the outcome variable for a new record with values of input variables. Linear regression and logistic regression are examples of supervised learning algorithms.\n",
    "\n",
    "# Unsupervised Learning:\n",
    "\n",
    "# In this case, the datasets will have only input variable values, but not the output. The algorithm learns the structure in the inputs. Clustering and factor analysis are examples of unsupervised learning.\n",
    "\n",
    "# How Machines Learn?\n",
    "\n",
    "# In supervised learning, the algorithm learns using a function called loss function, cost function, or error function, which is a function of predicted output and the desired output.\n",
    "\n",
    "# The objective is to learn the values of parameters (aka feature weights) that minimize the cost function. Machine learning uses optimization algorithms that can be used for minimizing the loss function. The most widely used optimization technique is called the Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   age                4521 non-null   int64 \n",
      " 1   job                4521 non-null   object\n",
      " 2   marital            4521 non-null   object\n",
      " 3   education          4521 non-null   object\n",
      " 4   default            4521 non-null   object\n",
      " 5   balance            4521 non-null   int64 \n",
      " 6   housing-loan       4521 non-null   object\n",
      " 7   personal-loan      4521 non-null   object\n",
      " 8   current-campaign   4521 non-null   int64 \n",
      " 9   previous-campaign  4521 non-null   int64 \n",
      " 10  subscribed         4521 non-null   object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 388.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# The implementation of the gradient descent algorithm in the previous section using numpy library is only for our understanding of how it works. But in practice, we will use the scikit-learn library in Python, which is primarily an open-source Python library for building machine learning models.\n",
    "\n",
    "# scikit-learn provides a comprehensive set of algorithms for the following kind of problems.\n",
    "\n",
    "# Regression\n",
    "# Classification\n",
    "# Clustering\n",
    "# scikit-learn also provides an extensive set of methods for data pre-processing and feature selection. \n",
    "\n",
    "# K-Fold Cross Validation:\n",
    "\n",
    "# K-fold cross-validation is a robust validation approach that can be adopted to verify if the model is overfitting. The model, which generalizes well and does not overfit, should not be very sensitive to any change in underlying training samples. K-fold cross-validation can do this by building and validating multiple models by resampling multiple training and validation sets from the original dataset.\n",
    "\n",
    "# Steps used for K- fold cross-validation: \n",
    "\n",
    "# Split the training data set into k equal subsets. Each subset is called a fold. Let the folds be named as  ùëì1,ùëì2,‚Ä¶,ùëìùëò  Generally, the value of k is taken to be 5 or 10.\n",
    "# For i = 1 to k\n",
    "# Keep the fold  ùëìùëñ as the validation set and all the remaining k-1 folds as the training set.\n",
    "# Train the machine learning model using the training set and calculate the accuracy of the model with the  ùëìùëñ fold.\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "df= pd.read_csv('../inputdata/bank.csv')\n",
    "df.head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One approach to deal with imbalanced data set is bootstrapping.\n",
    "# It involves resampling techniques called\n",
    "# up sampling and down sampling.\n",
    "# Up sampling: Up sampling increase the\n",
    "# instances of underrepresented\n",
    "# minority class in this case yes,\n",
    "# by replicating the existing observations in the data set.\n",
    "# Sampling with replacement is used for this purpose\n",
    "# and is called over sampling.\n",
    "# Down sampling reduce the instances of over represented\n",
    "# majority class by removing the\n",
    "# existing observations from the data set\n",
    "# and is also called under sampling.\n",
    "# Scikit learn utils has a resample\n",
    "# method to help with up sampling.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN stands for K-Nearest Neighbor and is a non-parametric,\n",
    "# lazy learning algorithm used for regression or classification problems.\n",
    "# KNN algorithm finds observations in training set,\n",
    "# which are similar to the new observation\n",
    "# that need to be classified or predicted.\n",
    "# These observations are called neighbors.\n",
    "# For better accuracy a set of neighbors not one many neighbors\n",
    "# which is given by the value K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Methods:\n",
    "\n",
    "# Ensemble methods, on the other hand, are learning algorithms that take a set of estimators or classifiers (models) and classify new data points using a strategy such as a majority vote. The majority voting could be based on counting simply the vote from each class or it could be weighted based on their individual accuracy measures. Ensemble methods are also used for regression problems, where the prediction of new data is simple average or weighted average of all the predictions from the set of regression models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
